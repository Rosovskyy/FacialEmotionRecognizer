{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-eaa4c7be144c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlab\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmlab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpathlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dlib'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "import dlib\n",
    "from pathlib import Path\n",
    "import sys, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProc:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.data = None\n",
    "        \n",
    "    def loadDataset(self, dirImg, formatFile, mapLabel):\n",
    "        lst = glob.glob(dirImg + \"/*.\" + formatFile)\n",
    "        labels, dataPath = [], []\n",
    "        l1, l1Norm = [], []\n",
    "        net = init_model_dnn()\n",
    "        strt_ind = lst[0]\n",
    "        for file in lst:\n",
    "            im = load_img(file, False, None)\n",
    "            detections = find_faces_dnn(im, net=net)\n",
    "            confidence_max_pos = np.argmax(detections[0, 0, :, 2])\n",
    "            if detections[0, 0, confidence_max_pos, 2] > self.conf_threshold:\n",
    "                (h, w) = im.shape[:2]\n",
    "                box = detections[0, 0, 0, 3:7] * np.array([w, h, w, h]).astype(\"int\")\n",
    "                l1.append(box)\n",
    "                l1.append(detections[0, 0, 0, 3:7])\n",
    "                dataPath.append(file)\n",
    "                labels.append(mapLabel[file[strt_ind : strt_ind + 2]])\n",
    "        l1 = np.array(l1)\n",
    "        l1Norm = np.array(l1Norm)\n",
    "        self._jaffe_data = {'file': data_path, 'label': labels, \n",
    "                            'x0' : l1[:, 0], 'y0' : l1[:, 1], \n",
    "                            'x1' : l1[:, 2], 'y1' : l1[:, 3]}\n",
    "        \n",
    "    def clear(self):\n",
    "        del self.data\n",
    "        self.data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haarCascadeLoad():\n",
    "    return cv2.CascadeClassifier('haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findFaces(img, cascades, param1=1.5, param2=5):\n",
    "    return cascades.detectMultiScale(img, param1, param2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutOutFaces(img, cascades, newSize):\n",
    "    faces = findFaces(img, cascades)\n",
    "    facesFound = []\n",
    "    for (x, y, w, h) in faces:\n",
    "        face = cv2.resize(img[y:y + h, x:x + w], newSize, interpolation=cv2.INTER_AREA)\n",
    "        facesFound.append(face)\n",
    "    return facesFound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haarFacesDB(xData, newSize=None):\n",
    "    cascades, newData = haarCascadeLoad(), []\n",
    "    for img in xData:\n",
    "        newData.append(np.array(cutOutFaces(np.array(img), cascades=cascades, newSize=newSize)))\n",
    "    return newData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutOutFacesDNN(im, net, newSize, conf_threshold=0.98):\n",
    "    faces_found = []\n",
    "    h, w = im.shape[:2]\n",
    "    net.setInput(cv2.dnn.blobFromImage(cv2.resize(im, (300, 300)), 1.0, (300, 300), [104, 117, 123], False, False))\n",
    "    for i in range(0, net.forward().shape[2]):\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "\n",
    "        if confidence > conf_threshold:\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            startX, startY, endX, endY = box.astype(\"int\")\n",
    "            face = cv2.cvtColor(im[startY:endY, startX:endX], cv2.COLOR_BGR2GRAY)\n",
    "            faces_found.append(cv2.resize(face, newSize, interpolation=cv2.INTER_AREA))\n",
    "\n",
    "    return np.array(faces_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findFacesDNN(im, net):\n",
    "    return net.setInput(cv2.dnn.blobFromImage(cv2.resize(im, (300, 300), interpolation=cv2.INTER_AREA), 1.0, (300, 300),\n",
    "                                 [104, 117, 123], False, False)).forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def facesFromDNNDB(xData, net, newSize=None, to_greyscale=False):\n",
    "    newData = []\n",
    "    for img in xData:\n",
    "        newData.append(np.array(cut_out_faces_dnn(np.array(img), net=net, newSize=newSize, to_greyscale=to_greyscale)))\n",
    "    return newData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD, Adam\n",
    "from keras.models import Sequential, model_from_json, Model \n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPool2D, MaxPooling2D, GlobalAveragePooling2D, GlobalMaxPool2D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.densenet import DenseNet121\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "from keras.applications.mobilenet import MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmNet:\n",
    "    def __init__(self, lossFunc, optim):\n",
    "        self.model = Sequential()\n",
    "        self.optim  = optim\n",
    "        self.lossFunc = lossFunc\n",
    "        \n",
    "    def model(self, shape, n):\n",
    "        self._model.add(Conv2D(32, (3, 3), input_shape = shape, activation = 'relu'))\n",
    "        self._model.add(Conv2D(32, (3, 3), input_shape = shape, activation = 'relu'))\n",
    "        self._model.add(Conv2D(32, (3, 3), input_shape = shape, activation = 'relu'))\n",
    "        self._model.add(BatchNormalization(epsilon=0.0001))\n",
    "        self._model.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2)))\n",
    "        \n",
    "        self._model.add(Conv2D(64, (3, 3), input_shape = shape, activation = 'relu'))\n",
    "        self._model.add(Conv2D(64, (3, 3), input_shape = shape, activation = 'relu'))\n",
    "        self._model.add(Conv2D(64, (3, 3), input_shape = shape, activation = 'relu'))\n",
    "        self._model.add(BatchNormalization(epsilon=0.0001))\n",
    "        self._model.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2)))\n",
    "        \n",
    "        self._model.add(Conv2D(64, (3, 3), input_shape = shape, activation = 'relu'))\n",
    "        self._model.add(Conv2D(64, (3, 3), input_shape = shape, activation = 'relu'))\n",
    "        self._model.add(Conv2D(64, (3, 3), input_shape = shape, activation = 'relu'))\n",
    "        self._model.add(BatchNormalization(epsilon=0.0001))\n",
    "        self._model.add(GlobalAveragePooling2D())\n",
    "        \n",
    "        self._model.add(Dense(512, activation = \"relu\"))\n",
    "        self._model.add(Dropout(0.5)) \n",
    "        self._model.add(Dense(256, activation = \"relu\"))\n",
    "        self._model.add(Dropout(0.5))\n",
    "        self._model.add(Dense(n, activation = 'softmax'))\n",
    "        \n",
    "    def train(self, x_train, y_train, batch, epochs, lossFunc, optim, save_best, save_best_to):\n",
    "\n",
    "        self.lossFunc = loss_func\n",
    "        self.optim  = optim\n",
    "        self.model.compile(loss = loss_func, optimizer = optim, metrics = [\"accuracy\"])\n",
    "\n",
    "        return self._model.fit(x_train, y_train, batch_size = batch, epochs = epochs, verbose = 1, shuffle = True, \n",
    "                        validation_split = 0.15)\n",
    "                    \n",
    "    def saveWeights(self, fileName):\n",
    "        self.model.save_weights(fileName)\n",
    "\n",
    "\n",
    "    def saveModel(self, fileName):\n",
    "        with open(fileName, 'w') as json_file:\n",
    "            json_file.write(model.to_json())\n",
    "\n",
    "    def predict(self, imgs):\n",
    "        self.model.compile(loss = self.lossFunc, optimizer = self.optim)\n",
    "        return self.model.predict(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(x_data, y_data, im_rows, im_cols, channels=1):\n",
    "        x_data = np.array(x_data, dtype='float32')\n",
    "        y_data = np.array(y_data, dtype='int32')\n",
    "        x_data = x_data.reshape((x_data - 123.0) / 123.0.shape[0], im_rows, im_cols, channels)\n",
    "        return x_data, np_utils.to_categorical(y_data, n_classes), np.unique(y_data).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(dataset_csv, \n",
    "                        im_shape, \n",
    "                        model_folder = 'models\\\\', \n",
    "                        augment=False,\n",
    "                        detect_face=True,\n",
    "                        model_id='', \n",
    "                        save_model_id = '',\n",
    "                        load_weights=True, \n",
    "                        plot_metrix=False, \n",
    "                        channels=1,\n",
    "                        epocs=50,\n",
    "                        batch_size=32,\n",
    "                        arc=0):\n",
    "\n",
    "        gray = True if channels==1 else False\n",
    "        im_rows, im_cols = im_shape\n",
    "        if detect_face:\n",
    "                x_data, y_data = load_dataset_and_cut_out_faces(dataset_csv, label_map, new_size=(im_rows, im_cols), greyscale=gray)\n",
    "        else:\n",
    "                x_data, y_data = load_dataset_no_face(dataset_csv, new_size=(im_rows, im_cols), greyscale=gray,\n",
    "                                label_map = {'anger':2, 'surprise':3, 'neutral':0, 'happiness':1, 'sadness':4})\n",
    "        input_shape = (im_rows, im_cols, channels)\n",
    "        x_data, y_data, n_classes = normalize_data(x_data, y_data, im_rows, im_cols, channels)\n",
    "\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.15, random_state=random.seed())\n",
    "\n",
    "        Model = Emotion_Net()\n",
    "\n",
    "        if load_weights:\n",
    "                Model.load_model(model_folder + model_id + \"model.json\")\n",
    "                Model.load_weights(model_folder + model_id + \"model.h5\")\n",
    "        else:\n",
    "                Model.init_model(input_shape, n_classes, arc=arc)\n",
    "        if augment:\n",
    "                history_call = Model.augment_and_train(x_train, y_train,\n",
    "                        save_best_to = model_folder + save_model_id + \"model.h5\", \\\n",
    "                        batch_size=batch_size, n_epochs=epocs, optim=Adam())\n",
    "        else:\n",
    "                history_call = Model.train(x_train, y_train, \n",
    "                        save_best=False,\n",
    "                        save_best_to = model_folder + save_model_id + \"model.h5\", \\\n",
    "                        batch_size=batch_size, n_epochs=epocs, optim=Adam())\n",
    "        Model.evaluate_accur(x_test, y_test)\n",
    "        if plot_metrix:\n",
    "                plt.subplot(2,1,1)\n",
    "                plt.title('Loss')\n",
    "                plt.plot(history_call.history['loss'], label='training loss')\n",
    "                plt.plot(history_call.history['val_loss'], label='testing loss')\n",
    "\n",
    "                plt.subplot(2,1,2)\n",
    "                plt.title('Accuracy')\n",
    "                plt.plot(history_call.history['acc'], label='training accuracy')\n",
    "                plt.plot(history_call.history['val_acc'], label='training accuracy')\n",
    "                \n",
    "                plt.legend()\n",
    "\n",
    "        Model.save_model(model_folder + save_model_id + \"model.json\")\n",
    "        Model.save_weights(model_folder + save_model_id + \"model.h5\")\n",
    "        return n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols = (144, 144)\n",
    "    channels = 3\n",
    "    n_classes = 5\n",
    "    n_classes = train_keras_model(os.path.join(ROOT_DIR, 'data\\\\dataset.csv'), \n",
    "                                im_shape=(rows, cols), \n",
    "                                channels=channels, \n",
    "                                model_folder = os.path.join(ROOT_DIR, 'saved_models\\\\'),\n",
    "                                epocs=60, \n",
    "                                batch_size=32,\n",
    "                                augment=False, \n",
    "                                detect_face=True,\n",
    "                                load_weights=False, \n",
    "                                model_id='resnet_', \n",
    "                                save_model_id='resnet_',\n",
    "                                plot_metrix=True, \n",
    "                                arc=2)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
